---
layout: post
title: 'A Visual Guide of Do-calculus'
---
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


This post focuses on visualizing **do-calculus**, which was introduced by [Judea Pearl](http://bayes.cs.ucla.edu/jp_home.html) in 1995 in his seminal paper ['Causal diagrams for empirical research'](http://bayes.cs.ucla.edu/R218-B.pdf). 
Do-calculus consists of three rules where each rule describes how a certain graphical conditional independence test (i.e., a d-separation criterion) ascertains the equality of two (causal) probabilities.
The three rules of do-calculus are as follows:

$$\begin{align*}
\textsf{(Rule 1)} & & P_{\mathbf{x}}(\mathbf{y} \mid \mathbf{w})            & =P_{\mathbf{x}}(\mathbf{y} \mid \mathbf{w},\mathbf{z}) & \textsf{ if } (\mathbf{Z} \perp \mathbf{Y} \mid \mathbf{X},\mathbf{W}) & \textsf{ in } \mathcal{G}_{\overline{\mathbf{X}}} \\
\textsf{(Rule 2)} & & P_{\mathbf{x}}(\mathbf{y} \mid \mathbf{w},\mathbf{z}) & =P_{\mathbf{x},\mathbf{z}}(\mathbf{y} \mid \mathbf{w}) & \textsf{ if } (\mathbf{Z} \perp \mathbf{Y} \mid \mathbf{X},\mathbf{W}) & \textsf{ in } \mathcal{G}_{\overline{\mathbf{X}}\underline{\mathbf{Z}}} \\
\textsf{(Rule 3)} & & P_{\mathbf{x}}(\mathbf{y} \mid \mathbf{w})            & =P_{\mathbf{x},\mathbf{z}}(\mathbf{y} \mid \mathbf{w}) & \textsf{ if } (\mathbf{Z} \perp \mathbf{Y} \mid \mathbf{X},\mathbf{W}) & \textsf{ in }\mathcal{G}_{\overline{\mathbf{X},\mathbf{Z}(\mathbf{W})}}
\end{align*}$$

where $$\mathbf{Z}(\mathbf{W})$$ is the set of $$\mathbf{Z}$$ that are not ancestors of any $$\mathbf{W}$$ in $$\mathcal{G}_{\overline{X}}$$. A subscript is used for a probability term to denote a do-operation, i.e., $$P_{\mathbf{x}}(\cdot\mid \cdot) = P(\cdot \mid \cdot, do(\mathbf{x}))$$ (the probability when variables $$\mathbf{X}$$ are held fixed to $$\mathbf{x}$$). A subscript next to a causal graph $$\mathcal{G}$$ manipulates the given graph: $$\overline{\mathbf{X}}$$ removes the edges incoming to $$\mathbf{X}$$ while $$\underline{\mathbf{Z}}$$ deletes the outgoing edges from $$\mathbf{Z}$$.


Rule 1 is about insertion/deletion of observations, which is the generalization of traditional conditional independence to interventional distributions. Rule 2 is about exchange between observations and actions --- when doing becomes seeing. Rule 3 is about insertion/deletion of actions, whether additional interventions have no effect on the given distribution.
Without deep understanding, one can *use* do-calculus by just applying the d-separation criterion in manipulated graphs. 


#### Simplifying do-calculus

For the sake of easier understanding of do-calculus, I am going to modify its form.
Since $$do(\mathbf{X}=\mathbf{x})$$ operation removes incoming edges onto $$\mathbf{X}$$, a path including $$X$$ in such a graph will look like $$\dotsb \gets X\to \dotsb $$. Then, conditioning on $$\mathbf{X}$$ always blocks the path. Hence, there will be no $$X\in \mathbf{X}$$ which will become the part of any d-connection path (active path) between $$Z\in \mathbf{Z}$$ and $$Y\in \mathbf{Y}$$, if exists. That is, we can simply work on an alternative graph $$\mathcal{H} = \mathcal{G}\setminus \mathbf{X}$$ --- imagine $$\mathcal{G}$$ with holes on $$\mathbf{X}$$ as shown below.

![](/assets/posts/gs.pdf){:width="630px" .center-image}

One useful trick here is thinking about living inside a do-world. Do-operation $$do(\mathbf{X}=\mathbf{x})$$ creates a submodel $$\mathcal{M}_{\mathbf{x}}$$ from the original model $$\mathcal{M}$$. By imagining $$\mathcal{M}_{\mathbf{x}}$$ as a default world, we employ $$\mathcal{H}$$ as a causal graph and $$Q=P_{\mathbf{x}}$$ as the observational distribution in the do-world. Then, the rules are a bit simplified.


$$\begin{align*}
\textsf{(Rule 1)} & & Q(\mathbf{y} \mid \mathbf{w})            & =Q(\mathbf{y} \mid \mathbf{w},\mathbf{z})   & \textsf{ if } (\mathbf{Z} \perp \mathbf{Y} \mid \mathbf{W}) & \textsf{ in } \mathcal{H} \\
\textsf{(Rule 2)} & & Q(\mathbf{y} \mid \mathbf{w},\mathbf{z}) & =Q_{\mathbf{z}}(\mathbf{y} \mid \mathbf{w}) & \textsf{ if } (\mathbf{Z} \perp \mathbf{Y} \mid \mathbf{W}) & \textsf{ in } \mathcal{H}_{\underline{\mathbf{Z}}} \\
\textsf{(Rule 3)} & & Q(\mathbf{y} \mid \mathbf{w})            & =Q_{\mathbf{z}}(\mathbf{y} \mid \mathbf{w}) & \textsf{ if } (\mathbf{Z} \perp \mathbf{Y} \mid \mathbf{W}) & \textsf{ in }\mathcal{H}_{\overline{\mathbf{Z}(\mathbf{W})}}
\end{align*}$$

where $$\mathbf{Z}(\mathbf{W})$$ is the subset of $$\mathbf{Z}$$ that are not ancestors of any $$\mathbf{W}$$ in $$\mathcal{H}$$.

For example, $$Z$$ and $$Y$$ are d-connected via a (back-door) path from $$Z\leftrightarrow \cdot \to W \leftrightarrow \cdot \to Y$$, which implies that 
none of the following is granted: 
$$P_{x}(y|w)= P_{x}(y|w,z)$$ (Rule 1),
$$P_{x}(y|w,z) P_{x,z}(y|w)$$ (Rule 2), 
$$P_{x}(y|w)=P_{x,z}(y|w)$$ (Rule 3).

### Visualizing the rules by drawing paths



Let's ignore $$\mathbf{X}$$ for now (for good).
To visualize do-calculus, first imagine an arbitrary causal graph. Testing for a conditional independence is checking whether there exists no d-connection path in $$\mathcal{H}$$.
One may categorize d-connection paths as either a *back-door* and *front-door* path. Focusing on $$Z\in\mathbf{Z}$$, if a path starts with $$Z\gets \dotsb Y$$, then it is called back-door path. Otherwise if $$Z\to \dotsb Y$$, then it is a front-door path.
Now, we connect these two types of paths with the three rules. For simplicity, let $$\mathbf{Z}=\{Z\}$$.

#### **Rule 1**



Now, Rule 1 looks fairly like a traditional conditional independence --- if there exists a d-connection path between $$Y\in\mathbf{Y}$$ and $$Z$$ given $$\mathbf{W}$$ in $$\mathcal{H}$$, one cannot simply add (or drop) $$Z$$ from the probability term. The figure below is quite an abstraction which illustrates two possible types of d-connection paths we care about. All other variables including $$\mathbf{W}$$ are implicit.

![](/assets/posts/rule1.pdf){:width="146px" .center-image }

This is a simple translation of conditional independence in the submodel created by an intervention. For sets $$\mathbf{Z}$$ and $$\mathbf{Y}$$, you can imagine superimposing them and whether there exists any d-connection path between any of $$Z\in\mathbf{Z}$$ and $$Y\in\mathbf{Y}$$.

#### **Rule 2**

To begin with, let me show you the equivalence between $$P_{\mathbf{x},z}(\mathbf{y}|\mathbf{w})$$ and $$P_{\mathbf{x},z}(\mathbf{y}|\mathbf{w},z)$$.
Intervening on $$Z=z$$ would generate data with $$Z=z$$. That is, filtering only on $$Z=z$$ by conditioning on $$z$$ as a condition has no effect on the probability term. Hence, the equality holds.

Although Rule 2 is described as the *exchange between observation and intervention*, 
it would be easier to interpret as whether adding an action $$do(z)$$ has an effect on the distribution when $$z$$ is given, i.e., comparing $$P_\mathbf{x}(\mathbf{y}|\mathbf{w},z)$$ and $$P_{\mathbf{x},z}(\mathbf{y}|\mathbf{w},z)$$. The test involves two different *worlds* (formerly, regimes).
A way to test is introducing an auxiliary variable $$\pi$$ pointing to $$Z$$ where $$\pi$$ can be viewed as a *mechanism selector* for $$Z$$.
That is, if $$\pi=0$$, $$f_Z$$ is used and $$\pi=1$$, $$f'_Z$$ is used (where $$f'_Z$$ can be just a constant! like $$do(z)$$).
With the auxiliary variable, now the question becomes $$P_\mathbf{x}(\mathbf{y}|\mathbf{w},z, \pi=0) = P_\mathbf{x}(\mathbf{y}|\mathbf{w},z, \pi=1)$$, or equivalently, $$P_\mathbf{x}(\mathbf{y}|\mathbf{w},z, \pi) = P_\mathbf{x}(\mathbf{y}|\mathbf{w},z)$$, i.e., 

$$(\pi \perp \mathbf{Y} \mid \mathbf{W},Z) \textsf{ in } \mathcal{H} \textsf{ with } \pi \textsf{ added.}$$


![](/assets/posts/rule2.pdf){:width="167px" .center-image }

Since $$Z$$ is blocked, front-door paths from $$Z$$ to $$Y\in\mathbf{Y}$$ should not be considered. We focus on finding a path $$\pi\to Z \gets \dotsb Y$$. That is $$Z \gets \dotsb Y$$, a back-door path from $$Z$$ to $$Y$$. Therefore, the test is translated to $$Z \perp \mathbf{Y} \mid \mathbf{W} $$ in $$\mathcal{H}_{\underline{Z}}$$.


Generalization to a set of $$\mathbf{Z}$$ is, unfortunately, not immediate. The test will be 

$$(\boldsymbol{\pi} \perp \mathbf{Y} \mid \mathbf{W},\mathbf{Z}) \textsf{ in } \mathcal{H} \textsf{ with } \boldsymbol{\pi} \textsf{ added.}$$

You might wonder Rule 2 does not include $$\mathbf{Z}$$ in the conditionals for its test.
Consider a back-door path from $$\pi_i$$ to some $$Y$$ which passes through $$Z_j$$ (or its ancestors as a collider) such that **not** having $$Z_j$$ as conditional would invalidate such path. This implies that $$Z_j$$ has a back-door path to $$Y$$. One can iteratively apply this idea to see that there must be $$Z\in \mathbf{Z}$$ that does not rely on $$\mathbf{Z}\setminus \{Z\}$$ as conditionals, hence, the CI test of Rule 2 test holds correctly. Let's see abstract figures.

![](/assets/posts/r2-special.pdf){:width="828px" .center-image }

In (a), the path from $$Z_1$$ to $$Y$$ through $$Z_2$$ implies that $$Z_2$$ has a back-door path to $$Y$$, which can be tested, which is irrelevant to $$Z_1$$ being given or not. In (b), the path is **not** because $$Z_2$$ is given but because $$W$$ is given. The back-door path will be valid without $$\mathbf{Z}$$ in conditional. 

An important case is shown in (c). There is no back-door path from $$\pi_1$$ to $$Y$$ since $$Z_2$$ is given nor from $$\pi_2$$ to $$Y$$. 
Then, it would be okay to elicit, e.g., $$P(y|z_1,z_2) = P_{z_1,z_2}(y) $$ through Rule 2. However, Rule 2 of do-calculus says that $$Z_1$$ has a back-door path to $$Y$$ since $$Z_2$$ is not given, prohibiting us from licensing the equality! Is there something wrong? Not really. Let's examine what's going on. Rule 2 only says that $$P(y|z_1,z_2) = P_{z_2}(y|z_1)$$. Then, applying Rule 2 again yields $$P_{z_2}(y|z_1) = P_{z_1,z_2}(y)$$. That is, the failure to apply Rule 2 *simply* does not imply that the two probabilities are *not* equal.

Then, does applying Rule 2 sequentially lead to the same conclusion based on the test with auxiliary variables? 
The answer is yes! Simply put, one can move an observation $$Z\in \mathbf{Z}$$ to an intervention if it does not involve $$\mathbf{Z}\setminus\{Z\}$$ in its path to some $$Y$$.
This can be iteratively applied to yield the same result based on the test with auxiliary variables.


#### **Rule 3**

Rule 3 is a bit tricky but the same idea is also applicable!
We employ the auxiliary variable $$\pi$$ for the change of mechanisms.
This time, the only difference is that $$Z$$ is not given.
Then, 

$$(\pi \perp \mathbf{Y} \mid \mathbf{W}) \textsf{ in } \mathcal{H} \textsf{ with } \pi \textsf{ added.}$$

Unlike the previous case (Rule 2) where $$Z$$ is given so that it blocks $$\pi\to Z \to $$ but opens $$\pi\to Z \gets $$,
this time we need to examine $$\mathbf{W}$$:

![](/assets/posts/rule3.pdf){:width="402px" .center-image}


If $$Z$$ is the part of the ancestor of $$\mathbf{W}$$, 
$$\pi\to Z \gets \dotsb$$ will be the start of a valid path since the descendant of $$Z$$ is given.
Regardless of $$\mathbf{W}$$, $$\pi\to Z \to \dotsb$$ is a valid path.
That is, if $$Z$$ is not an ancestor of $$\mathbf{W}$$, then we only care about front-door paths! A way to achieve this is ignoring back-door paths in $$\mathcal{H}$$. Hence, $$\mathcal{H}_{\overline{Z(\mathbf{W})}} \triangleq \mathcal{H}_{\overline{Z\setminus An(\mathbf{W})_{\mathcal{H}}}}$$.

If there exists no $$\mathbf{W}$$ is given, this is simplified to checking whether $$Z$$ are in the ancestors of $$\mathbf{Y}$$.
In $$\mathcal{G}$$ (not $$\mathcal{H}$$), this is checking whether $$\mathbf{X}$$ cuts directed paths from $$Z$$ to $$Y$$.

For sets $$\mathbf{Z}$$ and $$\mathbf{Y}$$, this corresponds to $$(\boldsymbol{\pi} \perp \mathbf{Y} \mid \mathbf{W}) \textsf{ in } \mathcal{H} \textsf{ with } \boldsymbol{\pi} \textsf{ added}$$. Since no $$\pi\in\boldsymbol{\pi}$$ appears in a possible path, this test translates well into $$(\mathbf{Z} \perp \mathbf{Y} \mid \mathbf{W}) \textsf{ in } \mathcal{H}_{\overline{\mathbf{Z}(\mathbf{W})}}$$.


